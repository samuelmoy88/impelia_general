{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[AI and Algorithmic Bias: Source, Detection, Mitigation and Implications](https://deliverypdf.ssrn.com/delivery.php?ID=394013103113028031116028106115064109050063050068079069007115005009071021107023120009034102098026110059062007088117094005007070045007060077040005025104085120113110024020033120001086005011027007082006084093103115074075007006022066013116006071003016067&EXT=pdf&INDEX=TRUE)\n",
    "\n",
    "El artículo \"AI and Algorithmic Bias: Source, Detection, Mitigation and Implications\" de Runshan Fu y colaboradores analiza cómo la inteligencia artificial y los algoritmos de aprendizaje automático, usados ampliamente en la toma de decisiones, han mostrado tendencias crecientes a perpetuar sesgos y desigualdades estructurales. Aborda la definición del sesgo algorítmico, los retos en su detección, las fuentes de este sesgo y revisa métodos para su corrección. Finalmente, el artículo discute cómo los algoritmos no sesgados pueden conducir a resultados sociales sesgados, concluyendo con preguntas abiertas y direcciones para futuras investigaciones.\n",
    "\n",
    "El artículo utiliza diversos métodos para abordar el sesgo algorítmico, incluyendo la pre-procesamiento de datos para eliminar sesgos antes del entrenamiento del algoritmo, la incorporación de restricciones de equidad durante el entrenamiento, y el post-procesamiento de las predicciones para corregir sesgos en los resultados. También se analizan métodos estadísticos y econométricos para la detección de sesgos, considerando la respuesta estratégica de los agentes a los algoritmos. Estos enfoques buscan equilibrar la precisión de las predicciones con consideraciones de equidad y justicia.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "[AI has a racism problem, but fixing it is complicated, say experts](https://www.cbc.ca/news/science/artificial-intelligence-racism-bias-1.6027150)\n",
    "\n",
    "El artículo de CBC News aborda el problema del racismo en la inteligencia artificial, destacando varios casos donde programas de IA han producido resultados racistas y sesgados. Ejemplos incluyen el uso de términos racistas en descripciones de productos en Amazon y traducciones sesgadas en Baidu. Expertos en IA explican que estos problemas surgen de cómo los algoritmos procesan grandes cantidades de datos de internet, que a menudo incluyen estereotipos y prejuicios. La solución al problema es complicada, ya que filtrar los datos para eliminar palabras y estereotipos racistas también podría censurar textos históricos y culturales relevantes. La discusión se centra en si los programas de IA deberían aprender por sí mismos o necesitar intervención humana para contrarrestar los sesgos.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "[Black Artists Say A.I. Shows Bias, With Algorithms Erasing Their History](https://www.nytimes.com/2023/07/04/arts/design/black-artists-bias-ai.html)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "[Racism And AI: Here’s How It’s Been Criticized For Amplifying Bias](https://www.forbes.com/sites/ariannajohnson/2023/05/25/racism-and-ai-heres-how-its-been-criticized-for-amplifying-bias/?sh=38a50fc9269d)\n",
    "\n",
    "El artículo destaca el problema del racismo en la inteligencia artificial, mostrando cómo esta puede perpetuar sesgos raciales en áreas como la salud, la aplicación de la ley y la tecnología. Se mencionan ejemplos de IA que no reconocen adecuadamente rostros de personas negras y cómo ciertas aplicaciones de reconocimiento facial han demostrado ser sesgadas. Se discute cómo la IA, alimentada por datos con prejuicios existentes, puede producir resultados sesgados y se plantea el debate sobre cómo abordar estos problemas. La necesidad de una mayor diversidad en los datos de entrenamiento de la IA y la intervención humana para corregir sesgos se subraya como crucial.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "[Sharing learnings about our image cropping algorithm](https://blog.twitter.com/engineering/en_us/topics/insights/2021/sharing-learnings-about-our-image-cropping-algorithm)\n",
    "\n",
    "Twitter recibió comentarios sobre su algoritmo de recorte de imágenes y su posible sesgo. La empresa realizó un análisis en colaboración con su equipo META y el equipo de investigación de contenido. Se evaluó el algoritmo para sesgos de género y raza y su alineación con el objetivo de permitir elecciones autónomas en la plataforma. Se encontraron diferencias en el recorte de imágenes basadas en género y raza, aunque no se evidenció sesgo de objetivación. Twitter decidió que la mejor solución es permitir a los usuarios controlar cómo se muestran sus imágenes, eliminando el recorte automático. La empresa está comprometida con la transparencia y la responsabilidad en el uso de sistemas de decisión algorítmica como el aprendizaje automático.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "[A.I. Bias Caused 80% Of Black Mortgage Applicants To Be Denied](https://www.forbes.com/sites/korihale/2021/09/02/ai-bias-caused-80-of-black-mortgage-applicants-to-be-denied/?sh=6da4e5aa36fe)\n",
    "\n",
    "Una investigación de The Markup reveló que los prestamistas son más propensos a rechazar solicitudes de préstamos hipotecarios a personas de color en comparación con solicitantes blancos de características financieras similares. El análisis señala que la IA en la originación de préstamos podría estar contribuyendo a estas disparidades. Se observó que el uso de software en las decisiones de préstamos hipotecarios, a menudo dictado por agencias gubernamentales, podría estar afectando negativamente la tasa de propiedad de viviendas entre afroamericanos. Las asociaciones bancarias han criticado este análisis, resaltando la complejidad de los detalles algorítmicos y las tendencias históricas en la propiedad de viviendas."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
